{"pages":[{"title":"404","text":"","link":"/404.html"},{"title":"","text":"--- layout:false --- google-site-verification: google180f97f51e406b66.html","link":"/google180f97f51e406b66.html"},{"title":"404","text":"","link":"/404/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"contact","text":"","link":"/contact/index.html"}],"posts":[{"title":"深度学习框架介绍","text":"在工作和学习中，一个成熟优秀稳定的框架能够帮我们节省不少事。 在此，本人将框架分为两种，一是基础的编程运行框架，二是在编程框架之上的应用框架 运行框架仅列举本人工作常用的运行框架 pytorch 学术界使用日渐广泛的一个框架 tensorflow-1 google提出的早期框架，仅支持静态图 tensorflow-2 tensorflow 2.0版本，动态图运行 paddle 百度提出的一个深度学习框架，配套有ppocr、pplite、ppslim等优秀的应用框架 应用框架pytorch相关 应用框架 说明 状态 备注 PyTorchLightning 一个优秀的用于高性能AI研究的轻量级PyTorch包装器 强烈推荐，能帮我们省不少事 timm 集成大部分backbone，并提供了统一的调用接口 推荐，使用十分方便 torchmetrics 集成诸多基于pytorch编写的metrics - cockpit 用于网络训练的调试工具 对代码需要有一定程度的修改 netron 查看ONNX网络结构 强烈推荐 gradsflow An open-source AutoML &amp; PyTorch Model Training Library 还处在开发阶段","link":"/shen-du-xue-xi/kuang-jia/readme/"},{"title":"","text":"资料：&lt;&lt;SQL必知必会&gt;&gt;一天学会MySQL数据库： https://www.bilibili.com/video/BV1Vt411z7wy/ 基础概念 数据库(database): 保存有组织的数据的容器(通常是一个文件或一组文件)。 最简单的办法是将数 据库想象为一个文件柜。这个文件柜是一个存放数据的物理位置，不管 数据是什么，也不管数据是如何组织的。 表(table)： 某种特定类型数据的结构化清单。 关键一点在于，存储在表中的数据是同一种类型的数据或清单。决不应该将顾客的清单与订单的清单存储在同一个数据库表中，否则以后的检索和访问会很困难。应该创建两个表，每个清单一个表。 表名: 数据库中的每个表都有一个名字来标识自己。这个名字是唯一的，即数据库中没有其他表具有相同的名字。模式: 关于数据库和表的布局及特性的信息列(column): 表中的一个字段。所有表都是由一个或多个列组成的行(row)：表中的一个记录主键(primary key)：一列(或一组列)，其值能够唯一标识表中每一行数据类型: 所允许的数据的类型。每个表列都有相应的数据类型，它限制(或允许)该列中存储的数据。 什么是 SQLSQL是 Structured Query Language(结构化查询语言)的缩写。SQL 是一种专门用来与数据库沟通的语言","link":"/sql/readme/"},{"title":"sql基础语法","text":"SQL (Structured Query Language)是一种用于编程的特定领域语言，用于管理关系数据库管理系统(RDBMS)中的数据，或者用于关系数据流管理系统(rdsm)中的流处理。它在处理结构化数据，即包含实体和变量之间关系的数据时特别有用。 基础语法 SQL 对大小写不敏感：SELECT 与 select 是相同的 某些数据库系统要求在每条 SQL 语句的末端使用分号。 在处理 SQL 语句时，其中所有空格都被忽略。因此SQL可以分写多行 数据库表一个数据库通常包含一个或多个表。每个表有一个名字标识（例如:”Websites”）,表包含带有数据的记录（行）。 use RUNOOB; # 命令用于选择数据库 set names utf8; # 命令用于设置使用的字符集 SELECT * FROM Websites; # 读取数据表的信息 通用数据类型 数据类型手册：https://www.runoob.com/sql/sql-datatypes-general.html 数据类型 描述 CHARACTER(n) 字符/字符串。固定长度 n。 VARCHAR(n) 或 CHARACTER VARYING(n) 字符/字符串。可变长度。最大长度 n。 INTEGER(p) 整数值（没有小数点）。精度 p。 SMALLINT 整数值（没有小数点）。精度 5。 INTEGER 整数值（没有小数点）。精度 10。 BIGINT 整数值（没有小数点）。精度 19。 FLOAT 近似数值，尾数精度 16。 DOUBLE PRECISION 近似数值，尾数精度 16。 DATE 存储年、月、日的值。 TIME 存储小时、分、秒的值。 BOOLEAN 存储 TRUE 或 FALSE 值 一些最重要的 SQL 命令 SELECT - 从数据库中提取数据 UPDATE - 更新数据库中的数据 DELETE - 从数据库中删除表中数据 INSERT INTO - 向数据库中插入新数据 CREATE DATABASE - 创建新数据库 ALTER DATABASE - 修改数据库 CREATE TABLE - 创建新表 ALTER TABLE - 变更（改变）数据库表 DROP TABLE - 删除表 CREATE INDEX - 创建索引（搜索键） DROP INDEX - 删除索引 命令详解 命令手册: https://www.runoob.com/sql/sql-quickref.htmlSQL网上训练: https://www.w3schools.com/sql/sql_syntax.asphttps://www.w3school.com.cn/sql/sql_func_count.asp SELECT 语法 SELECT column_name(s) FROM table_name WHERE condition AND|OR condition; 例子 SELECT * FROM Customers; SELECT CustomerID,CustomerName FROM Customers; SELECT DISTINCT 语法 SELECT DISTINCT column1, column2, ... FROM table_name; 例子 SELECT DISTINCT Country FROM Customers; SELECT DISTINCT Country,City FROM Customers; ### WHERE WHERE 子句用于过滤记录 - 语法 ```sql SELECT column_name,column_name FROM table_name WHERE condition; 例子 SELECT * FROM Customers WHERE Country='Mexico'; – 数值字段不要用引号SELECT * FROM CustomersWHERE CustomerID=1; ![](assets/2020-12-26-18-01-01.png) ### AND &amp; OR &amp; NOT AND &amp; OR &amp; NOT 运算符用于基于一个以上的条件对记录进行过滤 - 语法 ```sql SELECT column1, column2, ... FROM table_name WHERE condition1 AND condition2 AND condition3 ...; 例子 SELECT * FROM Customers WHERE Country='Germany' AND City='Berlin'; SELECT * FROM CustomersWHERE City=’Berlin’ OR City=’München’; SELECT * FROM CustomersWHERE NOT Country=’Germany’; SELECT * FROM CustomersWHERE Country=’Germany’ AND (City=’Berlin’ OR City=’München’); ### ORDER BY ORDER BY 关键字用于对结果集进行排序。 - 语法 ```sql -- 默认升序，可以使用DESC采用降序 SELECT column1, column2, ... FROM table_name ORDER BY column1, column2, ... ASC|DESC; 例子 SELECT * FROM Customers ORDER BY Country; SELECT * FROM CustomersORDER BY Country DESC; SELECT * FROM CustomersORDER BY Country, CustomerName; SELECT * FROM CustomersORDER BY Country ASC, CustomerName DESC; ### INSERT INTO INSERT INTO 语句用于向表中插入新记录。 - 语法 ```sql -- 第一种形式无需指定要插入数据的列名，只需提供被插入的值即可 INSERT INTO table_name VALUES (value1,value2,value3,...); -- 第二种形式需要指定列名及被插入的值 INSERT INTO table_name (column1,column2,column3,...) VALUES (value1,value2,value3,...); 例子 INSERT INTO Customers (CustomerName, ContactName, Address, City, PostalCode, Country) VALUES ('Cardinal', 'Tom B. Erichsen', 'Skagen 21', 'Stavanger', '4006', 'Norway'); INSERT INTO Customers (CustomerName, City, Country)VALUES (‘Cardinal’, ‘Stavanger’, ‘Norway’); ### UPDATE UPDATE 语句用于更新表中的记录。 - 语法 ```sql UPDATE table_name SET column1 = value1, column2 = value2, ... WHERE condition; 例子 UPDATE Customers SET ContactName = 'Alfred Schmidt', City= 'Frankfurt' WHERE CustomerID = 1; UPDATE CustomersSET ContactName=’Juan’WHERE Country=’Mexico’; UPDATE CustomersSET ContactName=’Juan’; ### DELETE DELETE 语句用于删除表中的记录。 - 语法 ```sql DELETE FROM table_name WHERE condition; 例子 DELETE FROM Customers WHERE CustomerName='Alfreds Futterkiste'; – 删除表中所有的行DELETE FROM Customers; ### 创建表 ```sql -- 创建一个空表 CREATE TABLE IF NOT EXISTS table_name_new ( item_id bigint, image_url string, ); -- 复制表结构及其数据 create table table_name_new as select * from table_name_old; -- 只复制表结构 create table table_name_new like table_name_old; -- 复制表数据到已有的新表中 （要求表的结构一致） -- 注意OVERWRITE和INTO的区别 INSERT INTO TABLE table_name_new SELECT * FROM table_name_old WHERE ds = '20201102'; 删除表DROP TABLE IF EXISTS table_name_new; 高级用法随机抽取n条记录SELECT * FROM tbcdm.dim_tb_itm_image WHERE ds = '20201202' ORDER BY RAND() LIMIT 30000; 统计表的行数COUNT()函数返回匹配指定条件的行数 SELECT COUNT(*) FROM tbcdm.dim_tb_itm_image WHERE ds = '20201202'; -- 排除重复值 SELECT COUNT(DISTINCT item_id) FROM tbcdm.dim_tb_itm_image WHERE ds = '20201202'; 字符串正则匹配语法： WHERE column LIKE pattern WHERE column NOT LIKE pattern 通配符 %： 匹配一个或者多个任意字符 _: 匹配一个任意字符 SELECT * FROM Websites WHERE url LIKE 'https%'; -- 搜索以https开头的url SELECT * FROM Websites WHERE url LIKE '%com'; -- 搜索以com结束的url","link":"/sql/ji-chu-yu-fa/"},{"title":"onnx profile性能分析","text":"为了做模型加速，有必要研究哪些处理需要花费多少时间，减少实际的处理时间。使用ONNX Runtime的配置函数进行配置，即可方便的查看onnx模型耗时瓶颈。 参考： https://linuxtut.com/en/659b99396e63e0378f78/ 代码 #coding=utf-8 import onnxruntime as rt import os import numpy as np class OnnxruntimeBackend(object): def __init__(self, model_path, profile_enable=False): if not model_path or not os.path.exists(model_path): raise ValueError(f'model_path {model_path} not exits') if profile_enable: options = rt.SessionOptions() options.enable_profiling = True # &lt;-Profile function enabled self.sess = rt.InferenceSession(model_path, options) else: self.sess = rt.InferenceSession(model_path) def __get_input_names__(self): &quot;&quot;&quot;Get input names of input Returns: dict: int-&gt;str, 模型输入的index和名称的对应关系 &quot;&quot;&quot; name_dict = {} for idx, input in enumerate(self.sess.get_inputs()): name_dict[idx] = input.name return name_dict def forward(self, images, input_mapping=dict()): &quot;&quot;&quot;onnx runtime model forward Args: images (np.asarray, NCHW): 输入的模型网络的图像数组, NCHW input_mapping (dict, optional): str-&gt;numpy.array,当存在多输入的情况，以k-v的形式表示模型输入]. Defaults to dict(). Returns: [list[np.asarray]]: 网络模型的推理输出 &quot;&quot;&quot; if not input_mapping: input_mapping = { self.sess.get_inputs()[0].name: images} onnx_outputs = self.sess.run(None, input_mapping) return onnx_outputs if __name__ =='__main__': images = np.ones((1,3,32,200), dtype=np.float32) model = OnnxruntimeBackend('***.onnx', True) # warmup output = model.forward(images) output = model.forward(images) output = model.forward(images) output = model.forward(images) prof_file = model.sess.end_profiling() print(prof_file) 查看报告打开Chrome浏览器，在地址栏输入 chrome://tracing 导入profiler生成的JSON文件： 按键盘w, a, s, d键，可以对profiler的结果进行缩放和移动","link":"/shen-du-xue-xi/gong-cheng/profiler-xing-neng-fen-xi/onnx-profiler/"},{"title":"分类loss汇总","text":"分类loss是最常接触到的loss，分类、检测、分割等多种任务都需要使用到分类loss。本文汇总一些常见的分类loss。 Cross Entropy无需多言，最经典的分类loss。 二分类交叉熵:$$L=-(ylog(\\hat{y})+(1-y)log(1-\\hat{y}))$$ 多分类交叉熵:$$L=-\\sum_{j=1}^{K}y^{(j)}log(\\hat{y}^{(j)})$$ 其中$y$是标签，$\\hat{y}$是预测的概率值,$K$是类别数 Weighted Cross-Entropy对于样本不平衡的问题，上述交叉熵损失很容易对样本多的类目过学习，而对样本少的类目欠学习。 为了缓解样本不平衡问题，可以对不同类目进行加权 二分类加权交叉熵: $$L=-(\\beta ylog(\\hat{y})+(1-\\beta)(1-y)log(1-\\hat{y}))$$ 多分类加权交叉熵:$$L=-\\sum_{j=1}^{K}w_j y^{(j)}log(\\hat{y}^{(j)})$$ 问题：权重如何设置呢？ 直观的想法是基于样本比例手动设置加权比例 更进一步是基于训练中的统计信息自适应加权 Focal Loss paper: Focal Loss for Dense Object Detection. 2017 加权交叉熵只是考虑类别不平衡，在同一类别中还需要考虑样本之间的难易程度，focal loss的目的就是关注难例样本(给难分类的样本较大权重) 二分类形式:$$L=-(\\alpha(1-p)^{\\gamma} plog(\\hat{p})+(1-\\alpha)p^{\\gamma}(1-p)log(1-\\hat{p}))$$多分类形式：$$L=-\\sum_{i=1}^{K}\\alpha_{i}(1-p)^{\\gamma} plog(\\hat{p})$$ 问题：$\\alpha$和$\\gamma$如何设置呢？ $\\alpha$是用来解决样本不均衡。通俗的说，某类的样本数越少，$\\alpha$的值越大 $\\gamma$是用来平衡难、易训练样本的系数。$\\gamma$越大，难样本loss得到的倾斜比例约大 总的来说，依然需要依靠经验手动调节$\\alpha$和$\\gamma$。对于$\\gamma$，作者的经验值是2；对于$\\alpha$，需要配合$\\gamma$调整 GHM paper: Gradient Harmonized Single-stage Detector.AAAI 2019 focal loss对难样本会重点学习，但现实情况往往会出现离群点（错标，漏标，污损等），对于这些离群点重点学习显然是不合适的 那么如何区分难样本和离群点呢？ 如下第一个子图所示，当模型接近收敛时，绝大部分样本的梯度很小，属于易分样本；一部分样本的梯度接近于1，并且数量不算少，GHM认为这批始终稳定存在的梯度特别大的样本，属于离群点。 由子图1推理出可以利用梯度密度来自适应对样本加权，既抑制梯度极端小的易样本，又抑制梯度极端大的离群点，同时避免了focal loss需要手动设置$\\alpha$和$\\gamma$的缺点。子图2为实现的加权函数。 现在的问题转为梯度密度的计算： 使用交叉熵损失函数时，一个样本的梯度的模为 $$g=|p-p^*|=\\begin{cases}1-p, &amp; p^*=1,\\p, &amp; p^*=0\\end{cases}$$ 计算梯度密度时，为了快速计算，直接划分10个bins，以每个bins的倒数作为权值加权 简化代码 # https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/losses/ghm_loss.py class GHMC(nn.Module): def __init__(self, bins=10, ......): self.bins = bins # tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,0.9000, 1.0000]) self.edges = torch.arange(bins + 1).float() / bins self.edges[-1] += 1e-6 def forward(self, pred, target): weights = torch.zeros_like(pred) tot = len(pred) g = torch.abs(pred.sigmoid().detach() - target) # 计算梯度模长 n = 0 # 有效的区间数 # 通过循环计算落入10个bins的梯度模长数量 for i in range(self.bins): inds = (g &gt;= self.edges[i]) &amp; (g &lt; self.edges[i + 1]) num_in_bin = inds.sum().item() if num_in_bin &gt; 0: weights[inds] = tot / num_in_bin # 梯度密度就是1/num_in_bin n += 1 if n &gt; 0: weights = weights / n loss = torch.nn.functional.binary_cross_entropy_with_logits( pred, target, weights, reduction='sum') / tot return loss ArcFace Loss paper: ArcFace: Additive Angular Margin Loss for Deep Face Recognition.2019 ArcFace属于人脸识别系列loss，也是当前人脸识别普遍使用的loss。 首先让我们回顾一下SoftmaxLoss,看看SoftmaxLoss在人脸识别领域的缺陷： $$L=-\\sum_{j=1}^{K}y^{(j)}log(\\hat{y}^{(j)})=-\\sum_{j=1}^{K}y^{(j)}log\\frac{e^{W^{T}{j}x+b_j}}{\\sum_{i=1}^{K}e^{W^{T}{i}x+b_{i}}}$$ 权重$W$将空间划分成K份，通过SoftmaxLoss学习到的特征$x$的2维映射如下，是一个扇形区域： 对于闭集任务（类别数目固定），Softmax训练的特征和$W$相乘之后，基于其所在的空间区域可以确定类别； 对于开集任务（类别数目不固定），我们并不是基于$W$去分类，而是基于样本特征的欧式距离或余弦距离判断样本是否属于同一类。Softmax训练的特征并不能保证类内样本的特征距离更小，类间样本特征距离更大。（如上图蓝色线和红色线的距离可能比两条红色线的距离要小） 人脸识别正是一个开集任务，直接利用Softmax学习可能不合适。 ArcFace首先将$W$和$x$归一化，忽略$b_j$的影响，$W^{T}x$即可以等价为一个$cos\\theta$,优化的目标是使类内样本与对应$W_j$夹角小，与其他类的$W_i$夹角大。权值和特征归一化使得CNN更加集中在优化夹角上，从而使得类内特征更聚合，类间特征更可分。最终的ArcFace Loss如下所示: $$L=-\\sum_{j=1}^{K}y^{(j)}log\\frac{e^{scos(\\theta_j+m)}}{e^{scos(\\theta_j+m)}+\\sum_{i=1,i\\neq j}^{K}e^{s*cos\\theta_i}}$$ 问题：$s$和$m$是什么，如何设置呢？ s可以视作超球面的半径，半径越大，特征表达的空间相应也更大;实质上就是一个缩放因子，$cos\\theta$值很小，对应的梯度也很小，加上一个缩放因子会更好优化。具体取值需要实验分析，取一个大于1的适中的数，论文给出的值为64 m是margin参数，作用是让权值向量$W$和特征向量$x$之间的夹角更小，迫使特征向量向$W$方向靠拢，论文给出的值是0.5 问题：ArcFace理论看起来很棒，可以用在其他非人脸识别任务上吗？理论上可以，但是目前ArcFace只在人脸识别领域广泛使用，在imagenet分类任务（闭集任务）应用比较少；在类似的ReID任务上也基本上没有什么应用，猜测可能是因为行人有不同的朝向和姿态，强行将这些差异较大的图像聚集在一个很小的角度内，很难学习的好。 最后查看ArcFace训练得到的可视化特征，可以看到Softmax是扇形区域，而ArcFace是聚焦在一个角度内。和ArcFace同系列的论文还有CosineFace、SphereFace等，工程实践表明ArcFace有相对更优的结果。 特征可视化 Loss分界面对比 简化代码 # https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/losses.py class ArcFace(nn.Module): def __init__(self, s=64.0, m=0.5): super(ArcFace, self).__init__() self.s = s self.m = m def forward(self, cosine: torch.Tensor, label): index = torch.where(label != -1)[0] m_hot = torch.zeros(index.size()[0], cosine.size()[1], device=cosine.device) m_hot.scatter_(1, label[index, None], self.m) cosine.acos_() cosine[index] += m_hot cosine.cos_().mul_(self.s) return cosine Circle Loss paper: Circle Loss: A Unified Perspective of Pair Similarity Optimization.2020 Circle Loss和上述介绍的ArcFace Loss类似，其本质不是去做一个纯分类任务（闭集分类），而是去学习到一个好的深度特征，该深度特征既要有小的类内距离，又要有大的类间距离。 通常有两种方式去学习得到这样的深度特征： 使用class-wise的标签进行分类学习（Softmax，CosFace，etc） 使用pair-wise的标签对比学习（triplet loss，N-Pair loss，etc） 在此之前，总是将这两种学习方式区别对待，该篇论文则从形式上将上述两种loss统一，并提出改进版的Circle Loss。 直接看统一形式的loss表达：$$L_{uni}=log[1+\\sum_{i=1}^{K}\\sum_{j=1}^{L}exp(\\gamma(s_n^{j}-s_p^{i}+m))]\\=log[1+\\sum_{j=1}^{L}exp(\\gamma(s_n^{j}+m))\\sum_{i=1}^{K}exp(\\gamma(-s_p^{i}))]$$ $K$为类内样本数，$L$为类间样本数，$s_p$表示类内样本相似度，$s_n$表示类间样本相似度， 改进后的circle loss（为方便阐述，公示中暂时省略m）： $$L_{circle}=log[1+\\sum_{i=1}^{K}\\sum_{j=1}^{L}exp(\\gamma(\\alpha_{n}^{j} s_n^{j}-\\alpha_p^i s_p^{i}))]\\=log[1+\\sum_{j=1}^{L}exp(\\gamma \\alpha_n^j s_n^{j})\\sum_{i=1}^{K}exp(-\\gamma \\alpha_p^i s_p^{i}))]$$ 问题一：统一形式的loss如何体现class-wise的分类loss和pair-wise的对比loss 对于class-wise的分类loss，特征$x$和分类层权重$w$的乘积表示相似度得分，此时$K=1$,$L=N-1$,$N$为类别数目 对于pair-wise的对比loss，$L_{uni}$就是triplet loss的一种通用写法。特别的，当$\\gamma\\rightarrow\\infty$时，$L_{uni}$退化为带hard mining的triplet loss；其他情况可以看着“soft” hard mining的triplet loss triplet loss和Softmax loss都是$L_{uni}$的一个特例，具体细节可以查看论文，不在此缀述 问题二：circle loss好像就是添加了$\\alpha_p$和$\\alpha_n$这两个系数，有什么作用呢 从$L_{uni}$可知，无论是class-wise的分类loss还是pair-wise的对比loss，都是在优化$s_n^{j}-s_p^{i}$这一项，配合下图能够直观的展示这一优化目标有两个问题： Lack of flexibility for optimization（优化缺乏灵活性）：当$s_p$很小，$s_n$几乎接近0时，合理的做法是去增大$s_p$，$s_n$保持稳定就好，但$L_{uni}$还是会对$s_n$施加一个较大的梯度惩罚。如（a）图中的A、B、C三个点对$s_p$和$s_n$的梯度是相同的。 Ambiguous convergence status（收敛状态模糊）：$T$和$T^{‘}$都在收敛边界$s_n-s_p=m$上，但是在$T^{‘}$上$s_n^{j+1}$和$T$上的$s_p^{i}$的距离可能要小于$m$ $\\alpha_p$和$\\alpha_n$作为两个调节系数，可以缓解上述两个问题，如图（b）所示： 优化更灵活：$s_n$很小的时候，$\\alpha_n$可以小一点，对$s_n$的惩罚强度也相应小一点，专注去优化增大$s_p$ 圆形收敛状态比斜线的收敛状态有更少的模糊性：$\\alpha_n s_n- \\alpha_p s_p=m$的边界是一个圆（这也是circleloss的由来） 问题三：和这两个调节系数如何设置呢 $\\alpha_p$、$\\alpha_n$和$s_p$、$s_n$ 直接相关，论文中给出了一个方便快捷的计算方式 $$L_{circle}=log[1+\\sum_{j=1}^{L}exp(\\gamma \\alpha_n^j (s_n^{j}-\\Delta_n))\\sum_{i=1}^{K}exp(-\\gamma \\alpha_p^i (s_p^{i}-\\Delta_p) ))]$$ $$\\begin{cases}\\alpha_p^i = [O_p - s_p^i]_+\\\\alpha_n^j = [s_n^j - O_n]_+\\end{cases}$$ $$O_p=1+m, O_n=-m, \\Delta_p=1-m, \\Delta_n=m$$ $\\Delta_p$和$\\Delta_n$是上文公式中忽略的m参数， $O_p$和$O_n$是$\\alpha_p$、$\\alpha_n$的理想值，经过简化后，整个circle loss只有两个超参数m和$\\gamma$ 实践表明，circle loss在人脸识别和reid等领域中验证均有正向效果，pair-wise + class-wise同时训练对reid可能会有更好的效果","link":"/shen-du-xue-xi/suan-fa/loss-hui-zong/fen-lei-loss/"},{"title":"","text":"LikeREGEXP_EXTRACT","link":"/sql/zheng-ze-pi-pei/"}],"tags":[{"name":"效率工具","slug":"效率工具","link":"/tags/%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"性能分析","slug":"性能分析","link":"/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"name":"loss","slug":"loss","link":"/tags/loss/"}],"categories":[{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"框架","slug":"深度学习/框架","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"},{"name":"sql","slug":"sql","link":"/categories/sql/"},{"name":"算法","slug":"深度学习/算法","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95/"},{"name":"loss","slug":"深度学习/算法/loss","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%AE%97%E6%B3%95/loss/"},{"name":"工程","slug":"深度学习/工程","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E7%A8%8B/"},{"name":"profiler性能分析","slug":"深度学习/工程/profiler性能分析","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E7%A8%8B/profiler%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"}]}